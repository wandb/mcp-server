"""
Pytest conftest.py for MCP Server test suite.

This file contains shared fixtures and hooks for the test suite, with a particular
focus on managing Weave evaluation logging in a distributed testing environment
using pytest-xdist.

Problem with pytest-xdist and session-level Weave Logging:
When using pytest-xdist for parallel test execution (e.g., `pytest -n <num_workers>`),
the `pytest_sessionfinish` hook is invoked by each worker process *and* by the
master process. If Weave evaluation logic (like initializing `weave.init()`,
instantiating `weave.EvaluationLogger`, collecting results, and logging a summary)
is placed naively within `pytest_sessionfinish`, it will run multiple times.
This leads to multiple, often identical or partial, Weave evaluations being logged
to the W&B project, which is undesirable.

Solution Implemented:
The `pytest_sessionfinish` hook in this conftest.py implements a mechanism to
ensure that the Weave evaluation aggregation and logging logic is executed *only*
once, by the master pytest process.

This is achieved by:
1. Determining if the current process is a worker or the master. Pytest-xdist
   provides information via `session.config.workerinput`. If `workerinput` is
   `None` or does not indicate a worker (e.g., `workerid` is not "gw0", "gw1", etc.),
   it's assumed to be the master process (or a non-xdist run).
   Specifically, we check `worker_id = session.config.workerinput.get('workerid', 'worker_unknown')`
   after confirming `session.config.workerinput` exists and is not None. The main aggregation
   logic then proceeds only if `worker_id == "master"`.
2. All Weave-related operations for summarizing the test run – including initializing
   Weave (`weave.init()`), creating the `EvaluationLogger` instance, discovering
   and parsing JSON result files generated by individual tests, and logging predictions
   and a final summary via the logger – are performed *only* within the conditional
   block for the master process.
3. Worker processes still run their individual tests and may generate intermediate
   result files (e.g., JSONs in their temporary directories), but they do not
   attempt to perform the final Weave aggregation or logging.

This approach ensures that a single, consolidated Weave evaluation is created
for the entire test session, even when tests are run in parallel across multiple
xdist workers.

Relevant libraries and versions during debugging of this issue:
- pytest: 8.3.5
- pytest-xdist: 3.6.1
- pluggy: 1.5.0
- weave: (version used by the project, e.g., 0.51.47 as seen in logs)
"""

import json
import logging
import os
import uuid
from collections import defaultdict
from datetime import datetime

import pytest
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Attempt to disable Weave tracing in worker processes by default.
os.environ["WEAVE_DISABLED"] = "true"

# Use standard logging for conftest, can be configured by main test suite logger if needed
logger = logging.getLogger("pytest.conftest")
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    # Basic formatter for conftest logs
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)

logger.info(f"Initial WEAVE_DISABLED set to: {os.environ.get('WEAVE_DISABLED')}")
os.environ["WANDB_SILENT"] = "true"  # To reduce W&B specific console output

# Try to import Weave and EvaluationLogger, allow failure if Weave is not installed or configured
try:
    import weave
    from weave import EvaluationLogger
    from weave.trace.context.weave_client_context import (
        WeaveInitError,
    )  # For specific error handling
except ImportError:
    weave = None
    EvaluationLogger = None
    WeaveInitError = Exception  # Fallback if WeaveInitError itself can't be imported
    logger.warning(
        "Weave SDK not found or could not be imported. Weave evaluation logging will be skipped."
    )

WANDB_TEST_SUITE_PROJECT = os.environ.get(
    "WANDB_PROJECT", "wandb-mcp-server-test-suite-outputs"
)
WANDB_TEST_SUITE_ENTITY = os.environ.get("WANDB_ENTITY", "wandb-applied-ai-team")


@pytest.fixture(scope="session", autouse=True)
def setup_weave_session_config(request):
    """Optional: Could be used for other session-wide non-Weave setup if needed."""
    logger.info(
        f"Pytest session starting. Target Weave project for aggregation: {WANDB_TEST_SUITE_ENTITY}/{WANDB_TEST_SUITE_PROJECT}"
    )
    # No weave.init() here. It will be handled by the master process in pytest_sessionfinish.


def pytest_configure(config):
    # Ensure asyncio event loop scope is appropriate if using async tests with pytest-asyncio
    # This was in your original file, keeping it for compatibility if needed.
    if hasattr(config.option, "asyncio_mode"):  # Check if pytest-asyncio options are present
        config.option.asyncio_mode = "auto"
        config.option.asyncio_default_fixture_loop_scope = "function"


class DateTimeEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)


WEAVE_RESULTS_DIR_NAME = "weave_eval_results_json"


@pytest.fixture(scope="session")
def weave_results_dir(tmp_path_factory):
    results_dir = tmp_path_factory.mktemp(WEAVE_RESULTS_DIR_NAME, numbered=False)
    logger.info(f"Session temp results directory created: {results_dir}")
    yield results_dir
    # Optional: Cleanup, though pytest tmp_path_factory usually handles it.
    # logger.info(f"Cleaning up session temp results directory: {results_dir}")
    # shutil.rmtree(results_dir, ignore_errors=True)


def pytest_sessionfinish(session):
    invocation_id = str(uuid.uuid4())

    worker_id = "master"
    workerinput = getattr(session.config, "workerinput", None)
    if workerinput is not None:
        worker_id = workerinput.get("workerid", "worker_unknown")

    logger.info(
        f"pytest_sessionfinish invoked (ID: {invocation_id}, ProcessID: {os.getpid()}, Worker: {worker_id})"
    )

    if worker_id == "master":
        logger.info(
            f"MASTER_LOGIC_RUN: Running main Weave aggregation logic in pytest_sessionfinish (ID: {invocation_id})"
        )

        original_weave_disabled_env = os.environ.get("WEAVE_DISABLED")
        logger.info(
            f"(ID: {invocation_id}) Original WEAVE_DISABLED state for master: {original_weave_disabled_env}"
        )
        os.environ["WEAVE_DISABLED"] = "false"
        logger.info(
            f"(ID: {invocation_id}) WEAVE_DISABLED temporarily set to 'false' for master's session_finish"
        )

        try:
            if not weave or not EvaluationLogger:
                logger.warning(
                    f"(ID: {invocation_id}) Weave SDK or EvaluationLogger not available. Skipping Weave summary logging."
                )
                return

            entity = WANDB_TEST_SUITE_ENTITY
            project = WANDB_TEST_SUITE_PROJECT

            if not entity or not project:
                logger.warning(
                    f"(ID: {invocation_id}) WANDB_ENTITY or WANDB_PROJECT not set. Skipping Weave summary logging."
                )
                return

            try:
                logger.info(
                    f"(ID: {invocation_id}) Initializing Weave in master session_finish for: entity='{entity}', project='{project}'"
                )
                weave.init(f"{entity}/{project}")
                logger.info(
                    f"(ID: {invocation_id}) Weave initialized successfully in master session_finish."
                )
            except WeaveInitError as wie:  # More specific error catching
                logger.error(
                    f"(ID: {invocation_id}) WeaveInitError initializing Weave: {wie}. Skipping summary logging.",
                    exc_info=True,
                )
                return
            except Exception as e:
                logger.error(
                    f"(ID: {invocation_id}) Generic error initializing Weave: {e}. Skipping summary logging.",
                    exc_info=True,
                )
                return

            all_test_data_raw = []
            json_files_found = []
            base_tmp_dir_for_master = None

            try:
                base_tmp_dir_for_master = session.config._tmp_path_factory.getbasetemp()
                logger.info(
                    f"(ID: {invocation_id}) Master session base temporary directory: {base_tmp_dir_for_master}"
                )
                for item_in_base_tmp_dir in base_tmp_dir_for_master.iterdir():
                    if item_in_base_tmp_dir.is_dir():
                        potential_results_parent_dir = item_in_base_tmp_dir
                        target_results_dir = (
                            potential_results_parent_dir / WEAVE_RESULTS_DIR_NAME
                        )
                        if target_results_dir.is_dir() and target_results_dir.exists():
                            logger.info(
                                f"(ID: {invocation_id}) Master searching for JSON files in: {target_results_dir}"
                            )
                            json_files_found.extend(
                                list(target_results_dir.glob("*.json"))
                            )
                        # Check if WEAVE_RESULTS_DIR_NAME was created directly under base_tmp_dir (non-xdist run)
                        elif (
                            item_in_base_tmp_dir.name == WEAVE_RESULTS_DIR_NAME
                            and item_in_base_tmp_dir.is_dir()
                        ):
                            logger.info(
                                f"(ID: {invocation_id}) Master searching for JSON files in non-xdist path: {item_in_base_tmp_dir}"
                            )
                            json_files_found.extend(
                                list(item_in_base_tmp_dir.glob("*.json"))
                            )

                json_files_found = sorted(
                    list(set(json_files_found))
                )  # Deduplicate and sort
            except Exception as e:
                logger.error(
                    f"(ID: {invocation_id}) Master error accessing/searching temp directories: {e}.",
                    exc_info=True,
                )
                return

            if not json_files_found:
                logger.info(
                    f"(ID: {invocation_id}) Master found no JSON result files. Searched in {base_tmp_dir_for_master if base_tmp_dir_for_master else 'unknown base path'}."
                )
                return

            logger.info(
                f"(ID: {invocation_id}) Master found {len(json_files_found)} JSON result files."
            )

            for json_file_path in json_files_found:
                try:
                    with open(json_file_path, "r") as f:
                        data = json.load(f)
                        all_test_data_raw.append(data)
                except Exception as e:
                    logger.error(
                        f"(ID: {invocation_id}) Master error reading/parsing {json_file_path}: {e}",
                        exc_info=True,
                    )
                    continue

            if not all_test_data_raw:
                logger.info(
                    f"(ID: {invocation_id}) Master: No valid data parsed from JSON files."
                )
                return

            # Group data by source_test_file_name
            grouped_test_data = defaultdict(list)
            for test_data_item in all_test_data_raw:
                source_file = test_data_item.get("metadata", {}).get(
                    "source_test_file_name", "unknown_source_file"
                )
                grouped_test_data[source_file].append(test_data_item)

            logger.info(
                f"(ID: {invocation_id}) Master: Processing {len(grouped_test_data)} groups of test data based on source file."
            )

            for source_file_name, file_specific_test_data in grouped_test_data.items():
                if not file_specific_test_data:
                    logger.warning(
                        f"(ID: {invocation_id}) Master: No test data found for source file group '{source_file_name}', skipping."
                    )
                    continue

                logger.info(
                    f"(ID: {invocation_id}) Master: Processing {len(file_specific_test_data)} tests from file '{source_file_name}'."
                )

                # Use metadata from the first test in this group for overall naming
                first_valid_metadata = file_specific_test_data[0].get("metadata", {})
                git_commit_id_from_tests = first_valid_metadata.get(
                    "git_commit_id", "unknown_git_commit"
                )

                # Sanitize source_file_name for use in eval name
                sanitized_source_name = source_file_name.replace("_", "-")

                overall_eval_name = (
                    f"mcp-eval_{sanitized_source_name}_{git_commit_id_from_tests}"
                )
                aggregated_dataset_name = f"{sanitized_source_name}_tests"

                logger.info(
                    f"(ID: {invocation_id}) Master: Logging to Weave Eval for file '{source_file_name}': Name='{overall_eval_name}', Commit='{git_commit_id_from_tests}', Dataset='{aggregated_dataset_name}'"
                )

                try:
                    session_eval_logger = EvaluationLogger(
                        name=overall_eval_name,
                        model=git_commit_id_from_tests,  # Or a more generic model identifier if preferred
                        dataset=aggregated_dataset_name,
                    )
                except Exception as e:
                    logger.error(
                        f"(ID: {invocation_id}) Master: Failed to init EvaluationLogger for '{source_file_name}': {e}.",
                        exc_info=True,
                    )
                    continue  # Skip to the next file group

                total_tests_logged = 0
                passed_tests_logged = 0
                all_latencies = []

                for (
                    test_data
                ) in file_specific_test_data:  # Iterate over tests for the current file
                    try:
                        metadata = test_data.get("metadata", {})
                        inputs = test_data.get("inputs", {})
                        output = test_data.get("output", {})
                        score_value = test_data.get("score", False)
                        metrics_data = test_data.get("metrics", {})
                        execution_latency = metrics_data.get(
                            "execution_latency_seconds"
                        )

                        current_inputs = dict(inputs)
                        if "test_case_index" in metadata:
                            current_inputs["_test_case_index"] = metadata[
                                "test_case_index"
                            ]
                        if "sample_name" in metadata:
                            current_inputs["_sample_name"] = metadata["sample_name"]
                        # _source_test_file_name is already known from the grouping key, but can be logged per prediction too
                        current_inputs["_source_test_file_name"] = metadata.get(
                            "source_test_file_name", source_file_name
                        )
                        current_inputs["_original_test_query_text"] = metadata.get(
                            "test_query_text", "N/A"
                        )  # Example of adding more metadata

                        score_logger = session_eval_logger.log_prediction(
                            inputs=current_inputs, output=output
                        )

                        score_logger.log_score(
                            scorer="test_passed", score=bool(score_value)
                        )

                        if execution_latency is not None:
                            score_logger.log_score(
                                scorer="execution_latency_seconds",
                                score=float(execution_latency),
                            )
                            all_latencies.append(float(execution_latency))

                        # Log other scores if present (e.g., from a 'scores' list in metrics_data or test_data)
                        # Example:
                        # other_scores = test_data.get('other_scores', []) # Assuming 'other_scores': [{'scorer': 'name', 'score': val}]
                        # for s in other_scores:
                        #    if 'scorer' in s and 'score' in s:
                        #        score_logger.log_score(scorer=s['scorer'], score=s['score'])

                        score_logger.finish()
                        total_tests_logged += 1
                        if score_value:
                            passed_tests_logged += 1
                    except Exception as e:
                        err_example_id = str(
                            metadata.get(
                                "test_case_index",
                                metadata.get("sample_name", "unknown_example"),
                            )
                        )
                        logger.error(
                            f"(ID: {invocation_id}) Master: Error logging prediction for '{err_example_id}' from file '{source_file_name}': {e}",
                            exc_info=True,
                        )

                if total_tests_logged > 0:
                    summary_metrics = {
                        "count_tests_logged": total_tests_logged,
                        "pass_rate": (passed_tests_logged / total_tests_logged)
                        if total_tests_logged
                        else 0,
                        "session_worker_count": str(
                            getattr(session.config.option, "numprocesses", "N/A")
                            if hasattr(session.config.option, "numprocesses")
                            else "N/A"
                        ),
                    }
                    if all_latencies:
                        summary_metrics["avg_execution_latency_s"] = sum(
                            all_latencies
                        ) / len(all_latencies)
                        summary_metrics["min_execution_latency_s"] = min(all_latencies)
                        summary_metrics["max_execution_latency_s"] = max(all_latencies)
                        summary_metrics["total_execution_latency_s"] = sum(
                            all_latencies
                        )

                    logger.info(
                        f"(ID: {invocation_id}) Master: Final Weave summary for '{overall_eval_name}': {summary_metrics}"
                    )
                    try:
                        session_eval_logger.log_summary(summary_metrics)
                        logger.info(
                            f"(ID: {invocation_id}) Master: Successfully logged summary for '{overall_eval_name}'."
                        )
                    except Exception as e:
                        logger.error(
                            f"(ID: {invocation_id}) Master: Failed to log summary for '{overall_eval_name}': {e}",
                            exc_info=True,
                        )
                else:
                    logger.info(
                        f"(ID: {invocation_id}) Master: No tests logged to Weave for '{overall_eval_name}', skipping summary."
                    )
            # End of loop for grouped_test_data

        finally:
            if original_weave_disabled_env is None:
                if os.environ.get("WEAVE_DISABLED") == "false":
                    del os.environ["WEAVE_DISABLED"]
            else:
                os.environ["WEAVE_DISABLED"] = original_weave_disabled_env
            logger.info(
                f"(ID: {invocation_id}) WEAVE_DISABLED restored by master to: {os.environ.get('WEAVE_DISABLED')}. Master session_finish part complete."
            )
    else:
        logger.info(
            f"WORKER_LOGIC_SKIP: Skipping main logic in pytest_sessionfinish for worker '{worker_id}' (ID: {invocation_id})"
        )
